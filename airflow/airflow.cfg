[core]
executor = CeleryExecutor
fernet_key = ' _c='
dags_folder = /opt/airflow/dags
load_examples = False
default_timezone = utc


[celery]
broker_url = redis://13.124.195.210:6379/0
result_backend = db+postgresql://airflow:airflow@13.124.195.210:5432/airflow
worker_concurrency = 16
celery_result_backend = db+postgresql://airflow:airflow@13.124.195.210:5432/airflow
task_default_queue = default
worker_prefetch_multiplier = 1
worker_enable_remote_control = True
flower_basic_auth = airflow:airflow

[webserver]
secret_key = 
base_url = http://13.124.195.210:8080
web_server_port = 8080
worker_refresh_batch_size = 1
worker_refresh_interval = 30
default_ui_timezone = utc
default_dag_run_display_number = 25
log_fetch_timeout_sec = 5
log_fetch_delay_sec = 2

[scheduler]
scheduler_heartbeat_sec = 5
min_file_process_interval = 30
dag_dir_list_interval = 300
max_threads = 4
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow
[logging]
base_log_folder = /opt/airflow/logs
remote_logging = False
[database]
sql_alchemy_pool_size = 5
sql_alchemy_pool_recycle = 1800
sql_alchemy_max_overflow = 10
sql_alchemy_pool_timeout = 30
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@13.124.195.210:5432/airflow

[secrets]
backend = airflow.providers.hashicorp.secrets.vault.VaultBackend
backend_kwargs = {"url": "http://127.0.0.1:8200", "token": "your_vault_token", "mount_point": "secret/data/airflow"}

[metrics]
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

