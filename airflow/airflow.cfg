[core]
executor = CeleryExecutor
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
fernet_key = ''
[logging]
# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /opt/airflow/logs

[celery]
broker_url = redis://localhost:6379/0
result_backend = db+postgresql://airflow:airflow@localhost:5432/airflow
# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

[webserver]
secret_key = temporary_key
# Path to a local folder where logs get temporarily written before
# sending them to remote storage.
# This folder must be absolute.
# If omitted or empty, defaults to 'base_log_folder'.
# When using 'gcs', provide a value to ensure logs are not pushed in local worker log folders.
remote_base_log_folder =

# Use server-side encryption for logs stored in s3
encrypt_s3_logs = False

# Log formats
# Default format for log lines
logging_config_class = airflow.utils.log.logging_mixin.DefaultLoggingConfig
